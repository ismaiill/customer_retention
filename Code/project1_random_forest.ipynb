{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"R_BL8ZDpgd4_"},"outputs":[],"source":["from google.colab import drive\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import roc_auc_score\n","from sklearn.impute import SimpleImputer\n","from sklearn.metrics import make_scorer\n","from sklearn.model_selection import GridSearchCV, StratifiedKFold"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5bXS9WYTg7Ip"},"outputs":[],"source":["drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LB9ItCFVhAc0"},"outputs":[],"source":["# Assuming that train.csv and test.csv are in the root of Google Drive\n","train_path = '/content/drive/My Drive/train.csv'\n","test_path = '/content/drive/My Drive/test.csv'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nybfnFG9hYsC"},"outputs":[],"source":["# Read the CSV files into pandas dataframes\n","train_df = pd.read_csv(train_path)\n","test_df = pd.read_csv(test_path)\n","original_test_df = pd.read_csv(test_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YjVu0wEgj6NU"},"outputs":[],"source":["# Convert the dataframes into numpy arrays\n","train_array = train_df.values\n","test_array = test_df.values"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":209,"status":"ok","timestamp":1707292405941,"user":{"displayName":"Jonayet Lavin","userId":"08382485236426425413"},"user_tz":480},"id":"48apKQScj8Ws","outputId":"ca1290ff-cd49-4ddc-b6c2-05f4dd98ba69"},"outputs":[{"name":"stdout","output_type":"stream","text":["(5343, 21)\n"]}],"source":["print(train_array.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MSkhpqfcie76"},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import make_scorer\n","\n","\n","# Drop the customerID column as it's not useful for prediction\n","train_df = train_df.drop('customerID', axis=1)\n","\n","# Fill in missing values of TotalCharges before encoding and scaling\n","train_df['TotalCharges'].fillna(train_df['TotalCharges'].median(), inplace=True)\n","\n","# Dictionary to hold the LabelEncoders\n","label_encoders = {}\n","\n","# Encode categorical features\n","categorical_features = [column for column in train_df.columns if train_df[column].dtype == 'object']  # type object means probably string text\n","for column in categorical_features: # categorical_features are features that can are deemed categorical\n","    le = LabelEncoder() # Converts categorical text data into a numerical format where each unique category/value in the column is assigned an integer\n","    train_df[column] = le.fit_transform(train_df[column]) # Learns what integers should be assigned to each unique value per column and reassigns the column\n","    label_encoders[column] = le  # Save the encoder for this column\n","\n","# Separate features and target\n","X = train_df.drop('Discontinued', axis=1)\n","y = train_df['Discontinued']\n","\n","# Split the dataset into training and validation sets\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Scale the features (to prevent feature bias)\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)  # Allows scalar to learn the scaling parameters (mean and standard deviatino) of the training set, which then scales training data to standard\n","X_val_scaled = scaler.transform(X_val)          # Uses same scaling parameters learned from training set\n","\n","# Define the parameter grid to search\n","param_grid = {\n","    'n_estimators': [100, 200, 300, 400, 500, 600],  # Number of trees in the random forest\n","    'max_depth': [10, 20, 30, 40, 50, None],  # Maximum number of levels in tree\n","    'min_samples_split': [2, 5, 10, 15, 20, 30],  # Minimum number of samples required to split a node\n","    'min_samples_leaf': [1, 2, 4, 6, 7, 8],  # Minimum number of samples required at each leaf node\n","}\n","\n","# Initialize the classifier\n","rf = RandomForestClassifier(random_state=42)\n","\n","# Create the grid search object\n","grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='roc_auc', n_jobs=-1)\n","\n","# Perform the grid search and fit the model\n","grid_search.fit(X_train_scaled, y_train)\n","\n","# Get the best estimator\n","best_rf = grid_search.best_estimator_\n","\n","# Predict probabilities for the validation set with the best model\n","y_pred_probs_best = best_rf.predict_proba(X_val_scaled)[:, 1]\n","\n","# Calculate ROC AUC with the best model\n","roc_auc_best = roc_auc_score(y_val, y_pred_probs_best)\n","print(f'Best ROC AUC Score: {roc_auc_best}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rPAuvnDZj-Ar"},"outputs":[],"source":["# Drop the customerID column as it's not useful for prediction\n","train_df = train_df.drop('customerID', axis=1)\n","\n","# Fill in missing values of TotalCharges before encoding and scaling\n","train_df['TotalCharges'].fillna(train_df['TotalCharges'].median(), inplace=True)\n","\n","# Dictionary to hold the LabelEncoders\n","label_encoders = {}\n","\n","# Encode categorical features\n","categorical_features = [column for column in train_df.columns if train_df[column].dtype == 'object']  # type object means probably string text\n","for column in categorical_features: # categorical_features are features that can are deemed categorical\n","    le = LabelEncoder() # Converts categorical text data into a numerical format where each unique category/value in the column is assigned an integer\n","    train_df[column] = le.fit_transform(train_df[column]) # Learns what integers should be assigned to each unique value per column and reassigns the column\n","    label_encoders[column] = le  # Save the encoder for this column\n","\n","# Separate features and target\n","X = train_df.drop('Discontinued', axis=1)\n","y = train_df['Discontinued']\n","\n","# Split the dataset into training and validation sets\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Scale the features (to prevent feature bias)\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)  # Allows scalar to learn the scaling parameters (mean and standard deviatino) of the training set, which then scales training data to standard\n","X_val_scaled = scaler.transform(X_val)          # Uses same scaling parameters learned from training set\n","\n","# Initialize and train the Random Forest Classifier with more trees and a specific maximum depth\n","rf_tuned = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n","rf_tuned.fit(X_train_scaled, y_train)\n","\n","# Predict probabilities for the validation set with the tuned model\n","y_pred_probs_tuned = rf_tuned.predict_proba(X_val_scaled)[:, 1]\n","\n","# Calculate ROC AUC with the tuned model\n","roc_auc_tuned = roc_auc_score(y_val, y_pred_probs_tuned)\n","print(f'Tuned ROC AUC Score: {roc_auc_tuned}')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ul073B1LiFkt"},"outputs":[],"source":["print(categorical_features[:-1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Y9zdwY_o2TQ"},"outputs":[],"source":["# TESTING\n","# Drop the customerID column as it's not useful for prediction\n","test_df = test_df.drop('customerID', axis=1)\n","\n","# Fill in missing values of TotalCharges\n","test_df['TotalCharges'].fillna(test_df['TotalCharges'].median(), inplace=True)\n","\n","# Exclude 'Discontinued' when listing categorical features\n","categorical_features = [column for column in categorical_features if column != 'Discontinued']\n","\n","# Use saved LabelEncoders to transform categorical features in the test set\n","for column in categorical_features:\n","    le = label_encoders[column]  # Retrieve the saved encoder for this column\n","    test_df[column] = le.transform(test_df[column])  # Transform test data using the saved encoder\n","\n","# Scale the features of the test set using the same scaler as the training set\n","X_test = test_df  # No need to drop 'Discontinued' as it's not present\n","X_test_scaled = scaler.transform(X_test)\n","# Predict probabilities for the test set\n","y_test_pred_probs = rf_tuned.predict_proba(X_test_scaled)[:, 1]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zaUplHpZgeJb"},"outputs":[],"source":["print(y_test_pred_probs)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5vH_u5aSix0z"},"outputs":[],"source":["# Output CSV\n","# Create a new dataframe with customerID and the predicted probabilities\n","output_df = pd.DataFrame({\n","    'ID': original_test_df['customerID'],\n","    'TARGET': y_test_pred_probs\n","})\n","# Save the new dataframe to a CSV file\n","output_df.to_csv('predicted_probabilities.csv', index=False)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}